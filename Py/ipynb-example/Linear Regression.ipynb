{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Python\n",
    "\n",
    "Linear regression in Python is available through the *sklearn* machine learning library. In this hands-on exercise we will examine the relationship between various attributes of blocks of households in California and the median house value for California districts. To begin let's import some libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy library provides us with some convenient ways to represent and manipulate vectors and matrices, which we will use extensively in this course. \n",
    "\n",
    "The pandas library provides very nice tools for managing datasets, while scipy is the Python scientific library with useful functions. \n",
    "\n",
    "The main star of this show is sklearn, which is the Python machine learning library, providing us with tools to create regression models using linear regression, and classification models using many models including Bayesian models and Support Vector Machines, which we will look at in this lecture.\n",
    "\n",
    "In the lab we will see how to use our own datasets, but for this lecture we will use some toy datasets that are given in sklearn. The toy datasets, amongst many others, include:\n",
    "\n",
    "- iris: Dataset on iris flowers for classification.\n",
    "- diabetes: A diabetes regression dataset.\n",
    "- digits: A collection of 8x8 images of digits for classification.\n",
    "- wines: Data on wine types for classification.\n",
    "- linnerrud: Dataset of physiological features of members of a fitness club for regression.\n",
    "- breast cancer: Breast cancer dataset for regression.\n",
    "\n",
    "Feel free to play around with the above datasets.\n",
    "\n",
    "## Loading the California housing dataset\n",
    "https://scikit-learn.org/stable/datasets\n",
    "\n",
    "For now we will use the California housing dataset. To load it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how big out dataset is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we see that there's 20640 pieces of data, with 8 attributes each. Let's look at what's in this dataset by printing the first 5 entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision = 3, suppress = True)\n",
    "d = repr(housing.data[:5]) # Nice printable version of housing.data\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see we have a Numpy array of 8 attributes, just as we found when we looked at the shape of the array. To see what each attribute does:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a very nice description of the 8 attributes that are available for regression. There is a 9th attribute (the target) which is the median house value for California districts, expressed in hundreds of thousands of dollars\n",
    "\n",
    "## Creating a Pandas Dataframe\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html\n",
    "\n",
    "To make it easier to manipulate the data, let's convert it to a Pandas dataframe, and print the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal = pd.DataFrame(housing.data)\n",
    "cal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice, but the columns don't have headings. This is set using the list cal.columns, and as it turns out the headings are available as a list in housing.feature_names. We can just set the cal.columns accordingly. You can also load just reload the dataset as a dataframe using  fetch_california_housing(as_frame = True)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal.columns = housing.feature_names\n",
    "cal.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now explore correlations between various columns of our data. For example, we may expect that a house with more rooms should have more bedrooms, or that the higher the median income of an area, the larger the house (i.e. more rooms). We make use of correlation to test these ideas. \n",
    "\n",
    "A correlation of 1 between two factors A and B means that factor A depends perfectly on factor B, while a correlation of -1 means that factor A is perfectly negatively correlated with factor B. We can find correlation using:\n",
    "\n",
    "```\n",
    "dataframe['A'].corr(dataframe['B'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "room_bedroom = cal['AveBedrms'].corr(cal['AveRooms'])\n",
    "age_pop = cal['HouseAge'].corr(cal['Population'])\n",
    "inc_room = cal['MedInc'].corr(cal['AveRooms'])\n",
    "\n",
    "\n",
    "print(\"corr rooms/bedrooms: %3.3f, corr house_age/population: %3.3f, corr median_income/rooms: %3.3f\" \n",
    "      % (room_bedroom, age_pop, inc_room))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, we see that the number of rooms and bedrooms have a strong correlation. We also see that there is some correlation between median icome and the average number of rooms. What is interesting is that population has an inverse correlation with house age. This is something worth looking at. :) But that's not why we are here.\n",
    "\n",
    "## Finding the Relationship Between Property Values and Median Income\n",
    "\n",
    "Let's see how we can create a linear regression model to find the relationship between property values and median income. In the  dataset, we can find the property values in housing.target. We first create a new column called \"Value\" in the Pandas dataframe, and then check that there is actually a correlation between property values and median income."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal['Value'] = housing.target\n",
    "print(\"Correlation between property values and median income: %3.3f\"\n",
    "     % cal['Value'].corr(cal['MedInc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see a moderate correlation between property values and median income, as we would expect. Let's start building our training data from the Pandas dataframe:\n",
    "\n",
    "### Creating the Data\n",
    "\n",
    "We will create the training data by extracting the target or \"dependent variable\" (property values) and the data driving the target (the \"independent variable\"). We then use linear regression to find the model relating independent and dependent variables.  We begin by converting both variables to from row vectors to column vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cal['MedInc'].values.reshape(-1, 1)\n",
    "Y = cal['Value'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into Training and Testing Data\n",
    "https://scikit-learn.org/stable/model_selection.html\n",
    "\n",
    "Before we create a model we always want to split the data into training data and testing (or validation) data. This allows us to test the model's \"goodness of fit\" against data it has seen for training, and data it has never seen before ('unseen data'). This is to test for \"overfitting\", where the model memorizes the training data and has excellent result, but produces very poor results for unseen data. We want to ensure that the results for both training data and testing data do not vary too greatly.\n",
    "\n",
    "We will use the train_test_split function in sklearn to put aside 33% of the data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.33)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will verify that we indeed have 33% of our data for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows = X.shape[0]\n",
    "test_rows = X_test.shape[0]\n",
    "print(\"%%age of data used for test: %3.2f%%\" % (test_rows / all_rows * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Regression Model\n",
    "\n",
    "Excellent! Now let's build our regression model. We start by importing the LinearRegression class from sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the fit method to learn from our data, and look at the coefficient and intercept for our model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm.fit(X_train, Y_train)\n",
    "print(\"Coefficient: %3.4f, Intercept: %3.4f.\" % (lm.coef_[0][0], lm.intercept_[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficient tells us how property values increase (since it is positive) for each increase in median income, while the intercept is a base value at $0 median income.  This gives us some insight into how median income effects property values (or maybe how property values effect median income). Note that you might get slightly different values here.\n",
    "\n",
    "### Evaluating for Over-fitting\n",
    "\n",
    "We briefly mentioned overfitting earlier in this document; a model has \"overfitted\" its training data if it can perform very well predicting the outputs for training data, yet perform very poorly on data it has never seen before.\n",
    "\n",
    "So now lets now look at how well this model fits the training data and the testing data. We will take the root-mean-square error (RMSE) given by:\n",
    "\n",
    "$$\n",
    "RMSE = \\sqrt{\\frac{\\sum_{i=0}^{n-1}(y_{a,i} - y_{m,i})^2}{n} }\n",
    "$$\n",
    "\n",
    "Here $y_{a,i}$ is the i-th *actual* value from the data, and $y_{m, i}$ is the corresponding output from the model.\n",
    "\n",
    "On its own the RMSE is fairly useless, but we can use it to compare the prediction error of the model when it is using training data it has seen, and testing data it has never seen.\n",
    "\n",
    "We call lm.predict(X_train) and lm.predict(X_test) to produce the predicted property values using the training and testing data respectively, then call the mean_squared_error function from the sklearn.metrics package and square-root the result. Recall that our target values are in Y_train and Y_test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "Y_pred_train = lm.predict(X_train)\n",
    "Y_pred_test = lm.predict(X_test)\n",
    "train_mse = np.sqrt(metrics.mean_squared_error(Y_train, Y_pred_train))\n",
    "test_mse = np.sqrt(metrics.mean_squared_error(Y_test, Y_pred_test))\n",
    "print(\"RMSE for training data: %3.4f. RMSE for testing data: %3.4f.\" % (train_mse, test_mse))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On its own the RMSE is fairly close. This means that the model performs about as well on data it has never seen, as on training data it has seen. So we are confident that the model is sufficiently \"general\" and has not memorized the training data to the detriment of unseen data.\n",
    "\n",
    "Note however that the RMSE does not give us a really good idea of how accurate our model is in absolute terms, just in relative terms. We can still use this however to compare against other models we create.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing with Our Model\n",
    "\n",
    "We can now make some predictions on our model. Note that we are not given the units of the median income, so we will assume it is in units of \\\\$10K here. We look at property values if the median income was \\\\$10K (in units of \\\\$10,000), versus values when the median income was \\\\$100K (in units of \\\\$10,000). We note that the values are in units of $100,000 and multiply accordingly.\n",
    "\n",
    "Note that lm.predict requires a 2D numpy array, which we create using np.array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value1 = lm.predict(np.array([[1]]))\n",
    "value2 = lm.predict(np.array([[10.0]]))\n",
    "\n",
    "print(\"Value at a median income of $10K is $%3.2f. Value at a median income of $100K is $%3.2f\"\n",
    "      % (value1[0][0] * 100000, value2[0][0] * 100000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we see that at a median income of \\\\$10K the house value is \\\\$85,587.68, which increases to \\\\$463,792.44 when the median income increases to \\\\$100K. We can manually verify that this is true:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = 0.4209 * 1 + 0.4398\n",
    "v2 = 0.4209 * 10 + 0.4398\n",
    "\n",
    "print(\"Value at a median income of $10K is $%3.2f. Value at a median income of $100K is $%3.2f\"\n",
    "      % (v1 * 100000, v2 * 100000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see an almost perfect match between the two models. :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
