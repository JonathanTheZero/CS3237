{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS3237 Lab 4 - Neural Networks and Deep Learning\n",
    "\n",
    "**Name:**\n",
    "\n",
    "**Student Number:**\n",
    "\n",
    "**Lab Group:**\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The objectives of this lab are:\n",
    "\n",
    "    1. To familiarize you with how to encode input and output vectors for neural networks.\n",
    "    2. To give you some insight into how hyperparameters like learning rate and momentum affect training.\n",
    "    3. To create, test and train, a CNN deep learning model using the MNIST dataset.\n",
    "    \n",
    "To save time we will train each experiment only for 10 epochs. This will lead to less than optimal results but is enough for you to make observations.\n",
    "\n",
    "## 2. The Irises Dataset\n",
    "\n",
    "We will now work again on the Irises Dataset, which we used in Lab 3, for classifying iris flowers into one of three possible types. As before we will consider four factors:\n",
    "\n",
    "    1. Sepal length in cm\n",
    "    2. Sepal width in cm\n",
    "    3. Petal length in cm\n",
    "    4. Petal width in cm\n",
    "\n",
    "In this dataset there are 150 sample points. The code below loads the dataset and prints the first 10 rows so we have an idea of what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of data:\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "print(\"First 10 rows of data:\")\n",
    "print(iris.data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Scaling the Data\n",
    "\n",
    "We make use of the MinMaxScaler to scale the inputs to between 0 and 1.  The code below does this and prints the first 10 rows again, to show us the difference.\n",
    "\n",
    "In the next section we will investigate what happens if we use unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 rows of SCALED data.\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.05084746 0.04166667]\n",
      " [0.08333333 0.45833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.66666667 0.06779661 0.04166667]\n",
      " [0.30555556 0.79166667 0.11864407 0.125     ]\n",
      " [0.08333333 0.58333333 0.06779661 0.08333333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
      " [0.02777778 0.375      0.06779661 0.04166667]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(iris.data)\n",
    "X = scaler.transform(iris.data)\n",
    "Y = iris.target\n",
    "\n",
    "print(\"First 10 rows of SCALED data.\")\n",
    "print(X[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoding the Targets\n",
    "\n",
    "In Lab 3 we saw that the target values (type of iris flower) is a vector from 0 to 2. We can see the 150 labels below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to train the neural network, but we will use \"one-hot\" encoding, where we have a vector of _n_ integers consisting of 0's and 1's.  The table below shows how one-hot encoding works:\n",
    "\n",
    "|   Value    |    One-Hot Encoding    |\n",
    "|:----------:|:----------------------:|\n",
    "| 0 | \\[1 0 0\\] |\n",
    "| 1 | \\[0 1 0\\] |\n",
    "| 2 | \\[0 0 1\\] |\n",
    "\n",
    "Pytorch provides the one_hot function to create one-hot vectors:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 1, 0],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1],\n",
      "        [0, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(F.one_hot(torch.tensor(iris.target), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's split the data into training and testing data:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size = 0.2, random_state = 1)\n",
    "train_x = torch.Tensor(train_x)\n",
    "train_y = F.one_hot(torch.tensor(train_y), 3).to(torch.float32)\n",
    "\n",
    "test_x = torch.Tensor(test_x)\n",
    "test_y = F.one_hot(torch.tensor(test_y), 3).to(torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset)\n",
    "test_loader = DataLoader(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Building our Neural Network\n",
    "\n",
    "Let's now begin building a simple neural network with a single hidden layer, using the Stochastic Gradient Descent (SGD) optimizer, ReLu transfer functions for the hidden layer and softmax for the output layer.\n",
    "\n",
    "The code to do this is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ModelNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelNN, self).__init__()\n",
    "        self.l1 = nn.Linear(4, 100)\n",
    "        self.l2 = nn.Linear(100, 3)\n",
    "    def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        output = self.l2(x)\n",
    "        return output\n",
    "\n",
    "model = ModelNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Training the Neural Network\n",
    "\n",
    "As is usually the case, we can call the \"fit\" method to train the neural network for 10 epochs. You can increase this to a larger value if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.0075\n",
      "Epoch [10/10], Loss: 0.0023\n",
      "Training complete!\n",
      "Training accuracy of the model: 90.0 %\n",
      "Test accuracy of the model: 93.33333333333333 %\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for input, labels in train_loader:\n",
    "        outputs = model(input)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, label = torch.max(labels, 1)\n",
    "        total += input.size(dim = 0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    print(f'Training accuracy of the model: {100 * correct / total} %')\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    i = 0\n",
    "    for input, labels in test_loader:\n",
    "        outputs = model(input)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, label = torch.max(labels, 1)\n",
    "        total += input.size(dim = 0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "    print(f'Test accuracy of the model: {100 * correct / total} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Question 1\n",
    "\n",
    "Run the code above. Do you see evidence of underfitting? Overfitting? Justify your answers. ***(4 MARKS)***\n",
    "\n",
    "**Overfitting would mean that the model performed way better on the training data than the test data. In my results the training accuracy is 90% and the testing accuracy is 93.3% so there is no overfitting here. Since both accuracies are fairly high, the model also did not underfit. Underfitting would mean that it performed pooly for both metrics as it hasn't learned enough from the data.**\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 4_\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 2a\n",
    "\n",
    "Consult the documentation for the SGD optimizer [here](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html). What does the lr parameter do? ***(1 MARK)***\n",
    "\n",
    "**Answer: Type your answer here. Do not hit return to continue to the next line, just let the text wrap around **\n",
    "\n",
    "#### Question 2b\n",
    "\n",
    "The momentum parameter \"accelerates gradient descent in the relevant direction and dampens oscillations\". Using Google or other means, illustrate what this means. ***(2 MARKS)***\n",
    "\n",
    "**Answer: Type your answer here. Do not hit return to continue to the next line, just let the text wrap around **\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 3_\n",
    "\n",
    "----\n",
    "\n",
    "#### Question 3a\n",
    "\n",
    "We will now play with the lr parameter. Adjust the lr parameter to the following values and record the final training and test accuracies in the respective columns. Also observe the sequence of accuracies over the training period, and place your observation in the \"remarks\" column, e.g. \"Progresses steadily\", \"some oscillation\" etc. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below **\n",
    "\n",
    "|  lr    | Training Acc. | Testing    Acc. |      Remarks      |\n",
    "|:------:|---------------|-----------------|-------------------|\n",
    "|0.01    |               |                 |                   |\n",
    "|0.1     |               |                 |                   |\n",
    "|1.0     |               |                 |                   |\n",
    "|10.0    |               |                 |                   |\n",
    "|100     |               |                 |                   |\n",
    "|1000    |               |                 |                   |\n",
    "|10000   |               |                 |                   |\n",
    "| 100000 |               |                 |                   |\n",
    "\n",
    "\n",
    "#### Question 3b\n",
    "\n",
    "Based on your observations above, comment on the effect of small and very large learning rates on the learning. ***(2 MARKS)***\n",
    "\n",
    "**Answer: Type your answer here. Do not hit return to continue to the next line, just let the text wrap around **\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "### 2.5 Using Momentum\n",
    "\n",
    "We will now experiment with the momentum term. To do this:\n",
    "\n",
    "    1. Change the learning rate to 0.1.\n",
    "    2. Set the momentum to 0.1. \n",
    "    \n",
    "Run your neural network.\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 4a\n",
    "\n",
    "Keeping the learning rate at 0.1, complete the table below using the momentum values shown. Again record any observations in the \"Remarks\" column. ***(3 MARKS)***\n",
    "\n",
    "**Answer: Fill the table below**\n",
    "\n",
    "| momentum | Training Acc. | Testing    Acc. |      Remarks      |\n",
    "|:--------:|---------------|-----------------|-------------------|\n",
    "|0.001     |               |                 |                   |\n",
    "|0.01      |               |                 |                   |\n",
    "|0.1       |               |                 |                   |\n",
    "|1.0       |               |                 |                   |\n",
    "\n",
    "#### Question 4b\n",
    "\n",
    "Based on your observations above, does the momentum term help in learning? ***(2 MARKS)***\n",
    "\n",
    "**Answer: Type your answer here. Do not hit return to continue to the next line, just let the text wrap around **\n",
    "\n",
    "_(For TA) Marks awarded: ____ / 5_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 4. Creating a CNN for the MNIST Dataset\n",
    "\n",
    "In this section we will now create a convolutional neural network (CNN) to classify images in the MNIST dataset that we used in Lecture 5. Let's build each part step by step.\n",
    "\n",
    "### 4.1 Loading the MNIST Dataset\n",
    "\n",
    "As in the Neural Network example from Lecture 5 we will load the MNIST dataset, scale the inputs to between 0 and 1, and convert the Y labels to one-hot vectors. However unlike before we will not flatten the 28x28 image to a 784 element vector, since CNNs can inherently handle 2D data. We will use a batch size of 1 for now, you may want to try other values for your training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "# Load the data, and normalise it.\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Change target to a one hot vector.\n",
    "target_transform = transforms.Compose(\n",
    "    [transforms.Lambda(lambda x: F.one_hot(torch.tensor(x), 10))])\n",
    "\n",
    "training_set = datasets.MNIST('data', train=True, download=True, transform=transform, target_transform=target_transform)\n",
    "test_set = datasets.MNIST('data', train=False, transform=transform, target_transform=target_transform)\n",
    "train_loader = torch.utils.data.DataLoader(training_set, batch_size = batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Create your CNN network\n",
    "\n",
    "Create your CNN network using pytorch (Hint: you may look at the CNN example from lecture 6). You should minimally have two convolutional layers, two maxpool layers, and at least one dense (linear) layer for the output. Use activation functions between layers such as `relu` or `softmax`. Pay careful attention to the size of the inputs and outputs. \n",
    "\n",
    "Write your class in the cell below and implement both the `constructor` and the `forward` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Enter your code for part 4.2 here in this code cell.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Train your CNN network\n",
    "\n",
    "Now train your network on the training dataset. You should train for 50 to 100 epochs.\n",
    "\n",
    "You will have to decide on which optimizer to use (SGD, adam, for example), which loss function you will use, and any other hyperparameters such as learning rate, batch size, and momentum.\n",
    "\n",
    "Write your code to train in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Enter your code for part 4.3 here in this code cell.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Test your CNN network\n",
    "\n",
    "Now test your network on the test dataset. Print out the accuracy of your model. \n",
    "\n",
    "Try modifying your model and choosing different hyperparameters and see if you can improve the accuracy of your model. \n",
    "\n",
    "Write your code to test in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Enter your code for part 4.4 here in this code cell.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5:\n",
    "\n",
    "Complete the following table with your final design (you may add more rows for the # neurons (layer1) etc. to detail how many neurons you have in each hidden layer). Likewise you may replace the lr, momentum etc rows with parameters more appropriate to the optimizer that you have chosen. (4 MARKS)\n",
    "\n",
    "| Hyperparameter       | What I used | Why?                  |\n",
    "|:---------------------|:------------|:----------------------|\n",
    "| Optimizer            |             |                       |\n",
    "| Input shape          |             |                       |\n",
    "| First layer          |             |                       |\n",
    "| Second layer         |             |                       |\n",
    "| Add more layers      |             |                       |\n",
    "| if needed            |             |                       |\n",
    "| Dense layer          |             |                       |\n",
    "| learning rate?       |             |                       |\n",
    "| momentum?            |             |                       |\n",
    "| loss function?       |             |                       |\n",
    "\n",
    "*FOR GRADER:* <br>\n",
    "*TABLE: _____ / 4* <br>\n",
    "*CODE: ______ / 10*<br>\n",
    "\n",
    "***TOTAL: ______ / 14 ***\n",
    "\n",
    "#### Question 6\n",
    "\n",
    "What is the final training and test accuracy that you obtained after 100 epochs. Are there signs of underfitting or overfitting? Explain your answer (4 MARKS)\n",
    "\n",
    "***Write your answer here***\n",
    "\n",
    "*FOR GRADER: ______ / 4*\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Conclusion\n",
    "\n",
    "In this lab we saw how to create a simple Dense neural network to complete the relatively simple task of learning how to classify irises according to their sepal and petal characteristics. We then tried using a CNN on the MNIST dataset. \n",
    "\n",
    "---\n",
    "\n",
    "***FOR TA ONLY***\n",
    "\n",
    "| Question |  Marks  |\n",
    "|:--------:|:-------:|\n",
    "|1         |     /4  |\n",
    "|2         |     /3  |\n",
    "|3         |     /5  |\n",
    "|4         |     /5  |\n",
    "|5         |     /14 |\n",
    "|6         |     /4  |\n",
    "|Total:    |     /35 |\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "055b62fcaec9821674a26809055da6bc29fe87d96b4c426e8bdfbe57b9f21334"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
