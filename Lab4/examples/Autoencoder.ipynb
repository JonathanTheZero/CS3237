{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Autoencoder\n",
    "\n",
    "In this notebook we will create a simple autoencoder that just learns a simple system represented by a linear function.\n",
    "\n",
    "We will use a simple dense network with Stochastic Gradient Descent optimizer. \n",
    "\n",
    "## Importing the Libraries.\n",
    "\n",
    "We begin by importing our libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple Autoencoder\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# For generating our fake data\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set some hyperparameters, including # of epochs to train, # of datapoints, as well as noise and # of testing data.\n",
    "\n",
    "Note that we have DATA_NOISE set at approximately 10% of our maximum and minimum data values, and TEST_NOISE set at 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs to train\n",
    "num_epochs = 200\n",
    "\n",
    "# Number of fake datapoints to create\n",
    "                                                                                                               \n",
    "DATA_SIZE = 10000\n",
    "DATA_NOISE = 0.1\n",
    "VAL_SIZE = int(0.2 * DATA_SIZE)\n",
    "\n",
    "# Test Noise level\n",
    "TEST_NOISE = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our Simple Autoencoder\n",
    "\n",
    "We will now create our autoencoder. We use a fully connected dense network with 3 hidden layers with 32, 16 and 32 hidden nodes. We use ReLu activation in all hidden layers and tanh for the output layer, which allows us to create and learn data from \\[-1, 1\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "   def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        # First hidden layer\n",
    "        self.l1 = nn.Linear(2, 32)\n",
    "        self.l2 = nn.Linear(32, 16)\n",
    "        self.l3 = nn.Linear(16, 32)\n",
    "        self.l4 = nn.Linear(32, 2)\n",
    "   def forward(self, x):\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l4(x)\n",
    "        output = F.tanh(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the Data\n",
    "\n",
    "Let's now generate the data! We set a fixed random seed (42), then create our random datapoints for X between 0 and 1. We then produce our Y data which goes from -1 to 1, and add some synthetic noise.\n",
    "\n",
    "Finally we call np.array to turn everything into NumPy arrays, which is needed by Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set a fixed seed so that our experiments are reproducible\n",
    "random.seed(42)\n",
    "\n",
    "# This adds white noise of -scale to scale\n",
    "def noise(scale):\n",
    "    return (2 * random.uniform(0, 1) - 1) * scale \n",
    "\n",
    "def gen_X(data_size):\n",
    "    return [random.uniform(0, 1) for i in range(data_size)]\n",
    "\n",
    "def gen_Y(X):\n",
    "    return [[x, (2*x + noise(DATA_NOISE) - 1)] for x in X]\n",
    "# We create our dummy dataset and output, with 1000 numbers\n",
    "X_in = gen_X(DATA_SIZE)\n",
    "X_test = gen_X(VAL_SIZE)\n",
    "\n",
    "# We create a simple linear function with noise for the network to learn\n",
    "Y = gen_Y(X_in)\n",
    "Y_out =  torch.tensor(Y)\n",
    "Y_test =  torch.tensor(gen_Y(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Noisy and Wrong Data\n",
    "\n",
    "We now create our very noisy data, as well as data that is produced using the wrong function ($1.5 \\times x$ instead of $2 \\times x$), so that we can evaluate how our simple autoencoder behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a noisy version of Y_out to check the anomaly detection\n",
    "Y_noisy = torch.tensor([[x, 2 * x + noise(TEST_NOISE) - 1] for x in X_in])\n",
    "\n",
    "# We create an incorrect version of Y_out\n",
    "Y_wrong = torch.tensor([[x, 1.5*x + noise(DATA_NOISE) - 1]for x in X_in])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation\n",
    "\n",
    "Finally let's train our autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train RMSE 0.0130, test RMSE 0.0128\n",
      "Epoch 50: train RMSE 0.0063, test RMSE 0.0062\n",
      "Epoch 100: train RMSE 0.0053, test RMSE 0.0052\n",
      "Epoch 150: train RMSE 0.0051, test RMSE 0.0049\n",
      "Epoch 200: train RMSE 0.0060, test RMSE 0.0059\n"
     ]
    }
   ],
   "source": [
    "model = Autoencoder()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "loader = data.DataLoader(data.TensorDataset(Y_out, Y_out), batch_size = 1)\n",
    "\n",
    "def train_model(model):\n",
    "    for epoch in range(num_epochs + 1):\n",
    "        model.train()\n",
    "        for dataX, dataY in loader:\n",
    "            y = model(dataX)\n",
    "            loss = loss_fn(y, dataY)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Validation\n",
    "        if epoch % 50 != 0:\n",
    "            continue\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(Y_out)\n",
    "            train_rmse = np.sqrt(loss_fn(y_pred, Y_out))\n",
    "            y_pred = model(Y_test)\n",
    "            test_rmse = np.sqrt(loss_fn(y_pred, Y_test))\n",
    "        print(\"Epoch %d: train RMSE %.4f, test RMSE %.4f\" % (epoch, train_rmse, test_rmse))\n",
    "\n",
    "train_model(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate the losses for the:\n",
    "\n",
    "    - Training data\n",
    "    - Test data\n",
    "    - Very noisy test data\n",
    "    - Incorrect data\n",
    "    \n",
    "From here we can see how the autoencoder behaves for each type of test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Clean loss = 0.0060, Test loss = 0.0059 Noisy loss = 0.0450, Wrong loss = 0.0681\n"
     ]
    }
   ],
   "source": [
    "def test_model(model, X, Y):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X)\n",
    "        test_rmse = np.sqrt(loss_fn(y_pred, Y))\n",
    "        return test_rmse\n",
    "\n",
    "\n",
    "clean_loss = test_model(model, Y_out, Y_out)\n",
    "test_loss = test_model(model, Y_test, Y_test)\n",
    "noisy_loss = test_model(model, Y_noisy, Y_noisy)\n",
    "wrong_loss = test_model(model, Y_wrong, Y_wrong)\n",
    "\n",
    "print(\"\\n\\nClean loss = %3.4f, Test loss = %3.4f Noisy loss = %3.4f, Wrong loss = %3.4f\" % \n",
    "(clean_loss, test_loss, noisy_loss, wrong_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "From the results above, we can see that the test loss is at 0.0059, while the noisy loss and wrong loss are a whole order of magnitude larger at 0.0450 and 0.0681. This allows us to set a threshold of perhaps 0.02 to determine when readings from a system (simulated here by a simple linear system) deviates too far from its normal operation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
